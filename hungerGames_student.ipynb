{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manularrea/Deep-Learning/blob/main/hungerGames_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX8zKrS-hRJk",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "# Práctica: Los Juegos del Hambre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XysGzsSAmBvu"
      },
      "source": [
        "ESTUDIANTE: `MANUELA LARREA, JERSON PEÑA`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vanYztAMhRJt",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "<table><tr>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/breakfast.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/hamburger.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "    <td><img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/fruits.jpg\" style=\"width:300px;height:300px;\"></td>\n",
        "</tr></table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6E7BW750hRJu",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "En esta práctica vamos a enfrentarnos a un problema desafiante de clasificación de imágenes, construyendo una red neuronal profunda que sea capaz de clasificar entre diferentes tipos de comida. ¡Que comiencen los Juegos del Hambre!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnn_FAm8hRJv",
        "nbgrader": {
          "grade": false,
          "locked": false,
          "solution": false
        }
      },
      "source": [
        "## Guidelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gspUM6n3hRJw"
      },
      "source": [
        "A lo largo del notebook encontrarás celdas que debes rellenar con tu propio código. Sigue las instrucciones del notebook y presta atención a los siguientes iconos:\n",
        "\n",
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Deberás resolver el ejercicio escribiendo tu propio código o respuesta en la celda inmediatamente inferior.</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZHQpQXrhRJw"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "Esto es una pista u observación de utilidad que puede ayudarte a resolver el ejercicio. Presta atención a estas pistas para comprender el ejercicio en mayor profundidad.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xghhJf_HhRJx"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "Este es un ejercicio avanzado que te puede ayudar a profundizar en el tema, y a conseguir una calificación más alta. ¡Buena suerte!</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWAbqrofhRJy"
      },
      "source": [
        "Para evitar problemas con imports o incompatibilidades se recomienda hacer uso [Google Colaboratory](https://colab.research.google.com/). Asegúrate de [conectar una GPU](https://colab.research.google.com/notebooks/gpu.ipynb), y de haber [deactivado otras sesiones que tuvieras activas](https://stackoverflow.com/a/53441194/2436578)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZmf8R9DhRJy"
      },
      "source": [
        "El siguiente código mostrará todas las gráficas en el propio notebook en lugar de generar una nueva ventana."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "DxmdgWqNhRJz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K733uf0ghRJ2"
      },
      "source": [
        "## Obtención de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZYPquwFhRJ2"
      },
      "source": [
        "Vamos a usar un dataset de imágenes de comida disponible en [Kaggle](https://www.kaggle.com/trolukovich/food11-image-dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRX8BQsZhRJ2"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Usa la celda inferior para descargar el dataset de Kaggle. ¡Ojo! Cuando entregues este notebook no dejes en él tus credenciales, ya que son personales a tu usuario de Kaggle.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wiFYSULfhRJ3",
        "outputId": "8ad3f976-e56d-42df-af6a-3a440a99a564"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/trolukovich/food11-image-dataset\n",
            "License(s): CC0-1.0\n",
            "Downloading food11-image-dataset.zip to /content\n",
            " 98% 1.06G/1.08G [00:11<00:00, 209MB/s]\n",
            "100% 1.08G/1.08G [00:11<00:00, 104MB/s]\n"
          ]
        }
      ],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"albarji\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"c98bec07523d01caaa7f117668cf22ce\"\n",
        "\n",
        "!kaggle datasets download trolukovich/food11-image-dataset --unzip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xmh27gIEhRJ4"
      },
      "source": [
        "Revisa ahora la carpeta en la que has descargado los datos. Verás que contiene 3 subdirectorios:\n",
        "\n",
        "* **training**, contiene las imágenes a utilizar para entrenar el modelo.\n",
        "* **validation**, contiene imágenes adicionales que podrías usar como datos de entrenamiento adicionales, o para algún tipo de estrategia de validación como Early Stopping.\n",
        "* **evaluation**, contiene las imágenes que debes utilizar para testear el modelo. Las imágenes de esta carpeta **solo** pueden utilizarse para medir el rendimiento del modelo tras su entrenamiento, y para nada más.\n",
        "\n",
        "Además de esto, dentro de cada una de estas carpetas encontrarás una subcarpeta para cada una de las 11 clases de comida:\n",
        "\n",
        "* Bread (panes)\n",
        "* Dairy product (lácteos)\n",
        "* Dessert (postres)\n",
        "* Egg (huevos)\n",
        "* Fried food (fritos)\n",
        "* Meat (carnes)\n",
        "* Noodles-Pasta (pasta)\n",
        "* Rice (arroz)\n",
        "* Seafood (pescado y marisco)\n",
        "* Soup (sopas)\n",
        "* Vegetable-Fruit (vegetales y frutas)\n",
        "\n",
        "Para facilitar los pasos de procesamiento que vendrán a continuación, vamos a definir algunas variables que nos indiquen dónde están almacenados los diferentes conjuntos de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJrE8F8zhRJ4"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Crea variables <b>TRAINDIR</b>, <b>VALDIR</b> y <b>TESTDIR</b>, cada una conteniendo una cadena de texto con la ruta al directorio donde están los datos de entrenamiento, validación y evaluación, respectivamente.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "TzK2VI3ShRJ5"
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "TRAINDIR, VALDIR, TESTDIR = 'training','validation', 'evaluation'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikYtQyR-hRJ6"
      },
      "source": [
        "### Reducción de clases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWsLot-7hRJ6"
      },
      "source": [
        "Con el fin de hacer este problema más accesible de cara a la práctica, vamos a centrarnos solo en seis de las clases de comida disponibles: `Bread`, `Dairy product`, `Dessert`, `Egg`, `Fried food` y `Meat`. Para ello, se provee el código siguiente, que elimina de los datos descargados las carpetas correspondientes a imágenes de las otras clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TXoNw1-thRJ7",
        "outputId": "2b2ed2d8-c9bb-4400-8faa-376dca4a57c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleting training/Soup...\n",
            "Deleting training/Noodles-Pasta...\n",
            "Deleting training/Rice...\n",
            "Deleting training/Vegetable-Fruit...\n",
            "Deleting training/Seafood...\n",
            "Deleting validation/Soup...\n",
            "Deleting validation/Noodles-Pasta...\n",
            "Deleting validation/Rice...\n",
            "Deleting validation/Vegetable-Fruit...\n",
            "Deleting validation/Seafood...\n",
            "Deleting evaluation/Soup...\n",
            "Deleting evaluation/Noodles-Pasta...\n",
            "Deleting evaluation/Rice...\n",
            "Deleting evaluation/Vegetable-Fruit...\n",
            "Deleting evaluation/Seafood...\n"
          ]
        }
      ],
      "source": [
        "from glob import glob\n",
        "import os\n",
        "\n",
        "valid_classes = {\"Bread\", \"Dairy product\", \"Dessert\", \"Egg\", \"Fried food\", \"Meat\"}\n",
        "datasets = {TRAINDIR, VALDIR, TESTDIR}\n",
        "\n",
        "for dataset in datasets:\n",
        "    for classdir in glob(f\"{dataset}/*\"):  # Find subfolders with classes\n",
        "        if classdir.split(\"/\")[-1] not in valid_classes:  # Ignore those in valid_classes\n",
        "            print(f\"Deleting {classdir}...\")\n",
        "            for fname in glob(f\"{classdir}/*.jpg\"):  # Remove each image file\n",
        "                os.remove(fname)\n",
        "            os.rmdir(classdir)  # Remove folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fncCfOYihRJ7"
      },
      "source": [
        "### Carga de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTQyBbf-hRJ-"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Utiliza la función <i>image_dataset_from_directory</i> para crear tres objetos de tipo Dataset, uno para cada una de las particiones de datos (training, validation, evaluation)\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDPUOXk62TPX"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "¿Cuál es el tamaño ideal al que cargar las imágenes en memoria?\n",
        "Esto es algo con lo que tendrás que experimentar a lo largo del notebook, por lo que probablemente tendrás que volver más de una vez a esta celda y probar cambiando ese tamaño.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lo más flexible aquí es tener una función lo suficientemente modular para poder experimentar con el tamaño de las imagenes (image_size)y el tamaño del lote (batch_size).\n",
        "\n",
        "Iniciamos con un experimento que toma tamaños de imagen de 32x32 y 64x64. Estos tamaños son los suficientemente pequeños para tener un procesamiento \"respetable\", y lo suficientemente grandes como para poder capturar características importantes de las imágenes."
      ],
      "metadata": {
        "id": "SaCPuXrbOso-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "D-wm19eJhRJ-"
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "def create_datasets(traindir = TRAINDIR, valdir = VALDIR, testdir = TESTDIR ,\n",
        "                    image_size = 32,\n",
        "                    batch_size = 64,):\n",
        "  label_mode = 'categorical'\n",
        "\n",
        "  train_dataset = image_dataset_from_directory(traindir,\n",
        "                                               image_size=(image_size,image_size),\n",
        "                                               batch_size=batch_size,\n",
        "                                               label_mode=label_mode\n",
        "                                               )\n",
        "\n",
        "  val_dataset = image_dataset_from_directory(valdir,\n",
        "                                               image_size=(image_size,image_size),\n",
        "                                               batch_size=batch_size,\n",
        "                                               label_mode=label_mode\n",
        "                                               )\n",
        "\n",
        "  test_dataset = image_dataset_from_directory(testdir,\n",
        "                                               image_size=(image_size,image_size),\n",
        "                                               batch_size=batch_size,\n",
        "                                               label_mode=label_mode\n",
        "                                               )\n",
        "\n",
        "  return train_dataset, val_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset, test_dataset = create_datasets()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KzZP4bLOQEf4",
        "outputId": "9dab7514-0adf-42e5-c90d-92bd6b2acdd0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 6082 files belonging to 6 classes.\n",
            "Found 2108 files belonging to 6 classes.\n",
            "Found 2070 files belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REESCALAR???"
      ],
      "metadata": {
        "id": "WVgIpYyMRsVm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kZnaNQIhRKB"
      },
      "source": [
        "## Construyendo tu propia red"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNd8dC2whRKB"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Diseña una red neuronal profunda que obtenga el mejor acierto posible sobre los datos de test. Puedes usar los datos de entrenamiento y validación como te parezca mejor, pero <b>sólo</b> puedes usar los datos de test para evaluar el acierto de tu modelo. Debes obtener una red capaz de alcanzar al menos un 45% de acierto sobre los datos de test.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9C9GNYihRKC"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "    \n",
        "Algunas recomendaciones y estrategias que pueden ayudar a mejorar tu diseño de red:\n",
        "    \n",
        "- <b>Arquitectura</b>: usa todos los trucos que has aprendido en los ejercicios anteriores: capas convolucionales + pooling, activaciones ReLU, dropout... asegúrate también de utilizar un buen optimizador, con una función de error (loss) adecuada, así como una función de activación en la capa de salida que sea adecuada para esta clase de problema (clasificación multiclase).\n",
        "- <b>Desarrollo incremental</b>: empieza por redes pequeñas, con un número pequeño de parámetros, de forma que puedas comprobar rápidamente qué tal funcionan. Después, puedes hacer tu red más grande en tres direcciones: mayor tamaño de imágenes de entrada, más capas, y más kernels por capa convolucional o unidades por capa densa. Si aumentas el tamaño de las imágenes de entrada, asegúrate de añadir también más capas Convolution+Pooling, para que así a la capa Flatten solo lleguen imágenes muy pequeñas (10x10 píxeles o menos).\n",
        "- <b>Tamaño de imágenes</b>: configurar los `Dataset` para que carguen imágenes de mayor tamaño puede mejorar significativamente el rendimiento de tu red. Pero ten cuidado, ¡también puedes encontrarte errores de falta de memoria (CUDA memory error) si cargas imágenes a un tamaño demasiado grande! Para esta práctica, un tamaño mayor a 256 puede ser demasiado grande...\n",
        "- <b>Controlar el número de épocas</b>: Usa una <a href=\"https://keras.io/api/callbacks/early_stopping/\">**estrategia de EarlyStopping**</a> para monitorizar el loss de los datos de validación, y así detener el entrenamiento cuando tras un número de épocas esa loss no haya decrecido. Configurar la EarlyStopping para restaurar los mejores parámetros encontrados durante la optimización también puede resultarte útil.\n",
        "- <b>Sobreajuste</b>: si tu red obtiene un accuracy casi perfecto en entrenamiento, puede que estés sufriendo sobreajuste (aunque puede que no...). Prueba a incrementar el nivel de Dropout en las capas Dense para comprobar si así obtienes mejoras en el test.\n",
        "- <b>Demasiado bueno para ser verdad</b>: si tu red obtiene resultados muy buenos, del orden del 90% o más de acierto en test... sospecha. Es posible que estés mezclando los datos de entrenamiento y test.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssDs0SGghRKC"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "    \n",
        "Como ejercicio avanzado, añade las siguiente estrategias a tu red:\n",
        "\n",
        "- Usa **técnicas de \"image augmentation\"** para aumentar artificialmente tu dataset de entrenamiento. Para ello, explora las <a href=\"https://keras.io/api/layers/preprocessing_layers/image_augmentation/\">capas de augmentation disponibles en Keras</a>.\n",
        "- Usa capas de <a href=\"https://keras.io/api/layers/normalization_layers/batch_normalization/\">BatchNormalization</a> para facilitar el entrenamiento de la red. Revisa en las diapositivas de clase cuál es la forma adecuada de colocarlas en la red.\n",
        "    \n",
        "Usando estos trucos y los mencionados en el punto anterior, es posible obtener más de un 60% de acierto en el conjunto de test.\n",
        "\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definición de la arquitectura:\n",
        "Vamos a seguir un diseño típico de red neuronal convolucional para problemas de clasificación de imagenes.\n",
        "\n",
        "Este diseño comienza con capas convolucionales y de agrupación para extraer características de las imagenes, luego tiene capas densas para clasificar esas características en las clases de interés.\n",
        "\n",
        "Usamos activaciones relu en las capas convolucionales y densas para introducir no linealidades y dorpout para regularización (para reducir sobreajuste)"
      ],
      "metadata": {
        "id": "tp6sg-HWSvQz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "Jgz7w3VqhRKC"
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def build_model(input_shape, num_classes):\n",
        "  model= Sequential()\n",
        "\n",
        "  # Capas convolucionales y de agrupación\n",
        "  model.add(Conv2D(32, (3,3), activation='relu', input_shape=input_shape))\n",
        "  model.add(MaxPooling2D((2,2)))\n",
        "  model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "  model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "  model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "  # Capa de aplanamiento y capas densas\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(256, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "  model.add(Dense(128, activation='relu'))\n",
        "  model.add(Dropout(0.5))\n",
        "\n",
        "  # Capa de salida\n",
        "  model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (64,64)\n",
        "num_classes = 6"
      ],
      "metadata": {
        "id": "DskP1HOhX6Yl"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Compilacion\n",
        "Usamos el famoso y muy usado optimizador Adam para entrenar redes neuronales.\n",
        "Como estamos haciendo una clasificación multiclase, establecemos la función de pérdida en categorical_crossentropy y monitoreamos la precisión con la métrica accuracy."
      ],
      "metadata": {
        "id": "vd1nAhsoV1lz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(input_shape=(image_size[0], image_size[1], 3),\n",
        "                    num_classes=num_classes)"
      ],
      "metadata": {
        "id": "ism4OpPUGGYl"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento con aumentación de imágenes\n",
        "Queremos introducir variaciones en los datos de entrenamiento (rotaciones, desplazamientos, zooms) para que el modelo se vuelva mas robusto a estas variaciones."
      ],
      "metadata": {
        "id": "VUMU_FJYYTPe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Generador de aumentación de imágenes\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "B2EOVeEWZDnt"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Iterar manualmente sobre train_dataset para obtener matrices NumPy de datos y etiquetas\n",
        "train_images = []\n",
        "train_labels = []\n",
        "\n",
        "for images, labels in train_dataset:\n",
        "    train_images.append(images.numpy())\n",
        "    train_labels.append(labels.numpy())\n",
        "\n",
        "train_images = np.concatenate(train_images)\n",
        "train_labels = np.concatenate(train_labels)\n"
      ],
      "metadata": {
        "id": "_J7PAx0-YpaP"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "vbnpdlkSabKH"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "co5LmoNZa8wq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = (64,64)\n",
        "batch_size = 32\n",
        "\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    TESTDIR,\n",
        "    image_size = (image_size[0], image_size[1]),\n",
        "    batch_size = batch_size,\n",
        "    label_mode = 'categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eZrVgxmbiU_5",
        "outputId": "fc9213ec-2ac3-458d-f799-1cf34ad93801"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2070 files belonging to 6 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Entrenar el modelo con aumentación de imágenes\n",
        "history=model.fit(\n",
        "    datagen.flow(train_images, train_labels),\n",
        "    epochs=50,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2057
        },
        "id": "aX_PYHFSaCtx",
        "outputId": "b4c35aae-2449-4d6a-fa2a-42184f549a35"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "Graph execution error:\n\nDetected at node sequential_2/dense_6/MatMul defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-54-6d26c58b6af8>\", line 2, in <cell line: 2>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/sequential.py\", line 398, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py\", line 241, in call\n\nMatrix size-incompatible: In[0]: [32,512], In[1]: [4608,256]\n\t [[{{node sequential_2/dense_6/MatMul}}]] [Op:__inference_train_function_5489]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-6d26c58b6af8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Entrenar el modelo con aumentación de imágenes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m history=model.fit(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdatagen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node sequential_2/dense_6/MatMul defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/usr/local/lib/python3.10/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n\n  File \"/usr/local/lib/python3.10/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelapp.py\", line 619, in start\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/platform/asyncio.py\", line 195, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 685, in <lambda>\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/ioloop.py\", line 738, in _run_callback\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 825, in inner\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 786, in run\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 361, in process_one\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 261, in dispatch_shell\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\", line 539, in execute_request\n\n  File \"/usr/local/lib/python3.10/dist-packages/tornado/gen.py\", line 234, in wrapper\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py\", line 302, in do_execute\n\n  File \"/usr/local/lib/python3.10/dist-packages/ipykernel/zmqshell.py\", line 539, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n\n  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n\n  File \"<ipython-input-54-6d26c58b6af8>\", line 2, in <cell line: 2>\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1807, in fit\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1150, in train_step\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 590, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/sequential.py\", line 398, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 515, in call\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/functional.py\", line 672, in _run_internal_graph\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/base_layer.py\", line 1149, in __call__\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 96, in error_handler\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py\", line 241, in call\n\nMatrix size-incompatible: In[0]: [32,512], In[1]: [4608,256]\n\t [[{{node sequential_2/dense_6/MatMul}}]] [Op:__inference_train_function_5489]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación\n",
        "Con el set de test se evalua el rendimiento del modelo... vamos a ver qué tal es la capacidad con datos no vistos"
      ],
      "metadata": {
        "id": "A4RoUSL9bNht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "print(f'Accuracy en el set de test: {test_accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jEXttCW4ba-q",
        "outputId": "4c2e2b28-8250-4fe1-e937-0cc0f609d6ed"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65/65 [==============================] - 3s 38ms/step - loss: 22.6188 - accuracy: 0.0952\n",
            "Accuracy en el set de test: 9.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrLe9mFwhRKC"
      },
      "source": [
        "## Red con transferencia del aprendizaje"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Sb7SVSkhRKH"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "    Usando la estrategia \"bottleneck\" implementa una red que haga transferencia del aprendizaje desde la red VGG16. Si lo haces correctamente, esta red debe obtener mejores resultados que la red que diseñaste en el apartado anterior, con al menos un 80% de acierto sobre el conjunto de test.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rm2LQa-mhRKI"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/exclamation.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#2655ad>\n",
        "    \n",
        "Algunos consejos para mejorar tu diseño de red:\n",
        "    \n",
        "- Incluye una o más capas Dense, con sus funciones de activación apropiadas, antes de la capa de salida.\n",
        "- Intenta usar una capa de tipo [GlobalAveragePooling](https://keras.io/api/layers/pooling_layers/global_average_pooling2d/) en lugar de la capa Flatten. Esta capa calcula una media de todos los valores de píxeles para cada canal, y en algunas ocasiones produce mejores resultados que la capa Flatten.\n",
        "- ¡Y no olvides todos los consejos del apartado anterior! También aplican aquí.\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl6bbHEWhRKI"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/pro.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "<font color=#259b4c>\n",
        "\n",
        "Para mejorar aún más los resultados de tu red, utiliza las siguientes ideas:\n",
        "\n",
        "- Usa las estrategias PRO del ejercicio anterior.\n",
        "- Prueba otras redes pre-entrenadas de <a href=\"https://keras.io/api/applications/\">Keras Applications</a>, como Xception o MobileNetV2, o alguna red de las familias EfficientNetV2 o ConvNeXt.\n",
        "- Usa una estrategia de transfer learning más avanzada, como fine-tuning o una combinación de bottleneck features y fine-tuning. Revisa las diapositivas de clase para saber cómo.\n",
        "   \n",
        "Si empleas todos estos trucos, es posible alcanzar más de un 90% de acierto en el conjunto de test.\n",
        "\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qLJ8IgANhRKI"
      },
      "outputs": [],
      "source": [
        "####### INSERT YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJc7DMJhRKK"
      },
      "source": [
        "## Informe final y resumen de resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV2c2JCKhRKK"
      },
      "source": [
        "<img src=\"https://albarji-labs-materials.s3-eu-west-1.amazonaws.com/question.png\" height=\"80\" width=\"80\" style=\"float: right;\"/>\n",
        "\n",
        "***\n",
        "\n",
        "<font color=#ad3e26>\n",
        "Escribe en la siguiente celda un pequeño informe con:\n",
        "    <ul>\n",
        "        <li>Una tabla de resultados, indicando qué diseños de red has probado y qué resultados en test has obtenido. Puede usar un estilo de tabla como el que se muestra abajo.</li>\n",
        "        <li>De las estrategias y diseños que has ido probando, ¿qué ha funcionado y qué no?</li>\n",
        "    </ul>\n",
        "</font>\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EofHCraehRKL"
      },
      "source": [
        "Ejemplo para la tabla de resultados\n",
        "\n",
        "|Procesado de imágenes|Diseño de red neuronal|Estrategia de entrenamiento|Acierto en test|\n",
        "|---------------------|----------------------|---------------------------|---------------|\n",
        "|Tamaño 32x32, batch size 16|Convolutional(32) + Flatten + Dense(64)|Entrenamiento desde 0|xx%|\n",
        "|Tamaño 64x64, batch size 32|VGG16 + Flatten + Dense(32)|Bottleneck features|yy%|\n",
        "|...|...|...|...|"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}